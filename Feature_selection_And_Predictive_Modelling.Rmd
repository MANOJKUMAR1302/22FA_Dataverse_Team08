---
title: "Feature selection - Prediction of resale car prices - Final porject"
author: "Yashwant Bhaidkar, Manojkumar Yerraguntla, Lakshmi Sravya Chalapati , Sai Charith Govardhanam"
date: "2022-11-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ezids)
library("dplyr")
library("plyr")
library(Boruta)
library(randomForest)
library(earth)
```

## Loading dataset

```{r results = T}
df = read.csv('cars.csv')
summary(df)
ncol(df)
```



## Checking null values and imputation
```{r , echo=FALSE}
apply(is.na(df),2,sum)
```


```{r}

df$engine_capacity[is.na(df$engine_capacity)]<-mean(df$engine_capacity,na.rm=TRUE)
apply(is.na(df),2,sum)
```


## converting categorical variable into factor

```{r}
new_df <- subset(df, select = -c(model_name,location_region))
new_df$transmission <-factor(new_df$transmission)
new_df$drivetrain <-factor(new_df$drivetrain)
new_df$manufacturer_name <- factor(new_df$manufacturer_name)
new_df$color <-factor(new_df$color)
new_df$engine_fuel <-factor(new_df$engine_fuel)
new_df$engine_type <-factor(new_df$engine_type)
new_df$engine_has_gas<-factor(new_df$engine_has_gas)
new_df$body_type <-factor(new_df$body_type)
new_df$has_warranty <-factor(new_df$has_warranty)
new_df$state <-factor(new_df$state)
new_df$drivetrain <-factor(new_df$drivetrain)
new_df$is_exchangeable<- factor(new_df$is_exchangeable)
names(new_df)[names(new_df) == "year_produced"] <- "Production_year"
```



```{r}
#applying 
feature_cols <- c(18:27)
new_df[feature_cols] <- (lapply(new_df[feature_cols],factor))
str(new_df)
```



```{r}
str(new_df)
```


```{r}
str(new_df)
```


```{r}
#In case of car brands, we have 55 different brands, so replacing them with median prices of the car.
avg_prices <-aggregate(new_df$price_usd, list(new_df$manufacturer_name), FUN=median)
colnames(avg_prices) <- c('manufacturer_name','average_brand_price')
avg_prices
```


```{r}
avg_prices$price_categories <- ifelse(avg_prices$average_brand_price <= 5000, 1, 
                   ifelse(avg_prices$average_brand_price <= 10000, 2, 
                       ifelse(avg_prices$average_brand_price <= 15000, 3, 
                            ifelse(avg_prices$average_brand_price <= 20000, 4, 5))))

avg_prices
```



```{r}
updated_df <- merge(new_df,avg_prices)
updated_df$price_categories <- factor(updated_df$price_categories)
str(updated_df)
```



```{r}
#scaling numerical features
updated_df <- rapply(updated_df,scale,c("numeric","integer"),how="replace")
str(updated_df)
```

```{r}
new_df <- updated_df
str(new_df)
```

### Feature Selection Methods

## Boruta Method

```{r}
# Applying the Boruta Method for the cars Dataset
bo <- Boruta(price_usd~.,data = new_df, doTrace = 2)
print(bo)
```

```{r}
# Plotting a boxplot for all the features according to it's importance
plot(bo,las = 2, cex.axis = 0.7)
```


## Feature selction using step wise regression method
We are using step-wise selection stategy

```{r}
#Stepwise regression
base.mod <- lm(price_usd ~ 1 , data= new_df)  # base intercept only model
all.mod <- lm(price_usd ~ . , data= new_df) # full model with all predictors
stepMod <- step(base.mod, scope = list(lower = base.mod, upper = all.mod), direction = "both", trace = 0, steps = 1000)  # perform step-wise algorithm
shortlistedVars <- names(unlist(stepMod[[1]])) # get the shortlisted variable.
shortlistedVars <- shortlistedVars[!shortlistedVars %in% "(Intercept)"]  # remove intercept 
print(shortlistedVars)
```

#MARS Method
```{r}
marsModel <- earth(price_usd ~ . , data = new_df) # build model
ev <- evimp (marsModel) # estimate variable importance
ev
```

```{r}
plot(ev)
```


## Random forest using cforest method


```{r}
#Approach 1 - Using cforest
library(party)
cf1 <- cforest(price_usd ~ . , data= new_df, control=cforest_unbiased(mtry=2,ntree=100)) # fit the random forest
varimp(cf1)
```


```{r}
cf_partykit_st <- stablelearner::as.stabletree(cf1)
summary(cf_partykit_st, original = FALSE)
barplot(cf_partykit_st)
```


```{r}
image(cf_partykit_st, cex.names = 0.6)
```




```{r}
####### Approach 2: random forest for feature selection
new_df_filtered <- subset(new_df,select = -c(manufacturer_name))
model_rf_base <- randomForest(price_usd~.,data = new_df_filtered,ntree=100,keep.forest=FALSE, importance=TRUE,do.trace = TRUE)
print(model_rf_base)
```



```{r}
library(ggplot2)
ImpData <- as.data.frame(importance(model_rf_base))
ImpData$Var.Names <- row.names(ImpData)

ggplot(ImpData, aes(x=Var.Names, y=`%IncMSE`)) +
  geom_segment( aes(x=Var.Names, xend=Var.Names, y=0, yend=`%IncMSE`), color="skyblue") +
  geom_point(aes(size = IncNodePurity), color="blue", alpha=0.6) +
  theme_light() +
  coord_flip() +
  theme(legend.position="bottom",
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank())
```


```{r}
#Best features:
# 
# Total_years
# Engine capacity
# drivetrain
# odometer_value
# Manufacturer
# body_type
# Feature_6
# Feature_7
# state
# Engine_type
# transmission
# number of photos
# color
# feature_3
# feature_5
```

### Predictive Modeling

```{r}
# The Predictive Models used in this project are:
# 1. Linear Regression
# 2. Lasso and Ridge Regression
# 3. SVM Regression
# 4. Random Forest Regression
# 5. XG Boost

```

## Linear regression

```{r}
#######regression model
model_linear_regression <- lm(price_usd~transmission + color + odometer_value + engine_type +
    engine_capacity + body_type + drivetrain + state + number_of_photos +
    feature_7 + feature_3 + feature_6 + feature_5 +
    Production_year + manufacturer_name,data = train_df)
summary(model_linear_regression)
```



```{r}
prediction_reg <- predict(model_linear_regression,newdata = train_df)
mod_train <- data.frame(Dataset_used = 'Train',RMSE = RMSE(prediction_reg,train_df$price_usd),Rsquared = R2(prediction_reg,train_df$price_usd))
mod_train
```



```{r}
prediction_reg <- predict(model_linear_regression,newdata = test_df)
mod_test <- data.frame(Dataset_used = 'test',RMSE = RMSE(prediction_reg,test_df$price_usd),Rsquared = R2(prediction_reg,test_df$price_usd))
mod_test
```


```{r}
final <- rbind(mod_train,mod_test)
final
```

```{r}
library(caret)
index <- createDataPartition(updated_df$price_usd, p = 0.8, list = FALSE,times = 1)

new_df <- as.data.frame(updated_df)

train_df <- updated_df[index,]
test_df <- updated_df[-index,]
```

```{r}
#######################Lasso regression##################################
ctrls <- trainControl(method = 'cv',number = 10,savePredictions = 'all')
```


```{r}
lambda_vect <- 10^seq(5,-5,length = 500)
model_lasso  <- train(price_usd~transmission + color + odometer_value + engine_type +
    engine_capacity + body_type + drivetrain + state + number_of_photos +
    feature_7 + feature_3 + feature_6 + feature_5 +
    year_produced + manufacturer_name,data = updated_df,
    method = "glmnet",tuneGrid = expand.grid(alpha = 1,lambda = lambda_vect),trControl = ctrls)
```

```{r}
model_lasso$bestTune
```

```{r}
round(coef(model_lasso$finalModel,model_lasso$bestTune$lambda),3)
```

```{r}
prediction1 <- predict(model_lasso,newdata = train_df)
mod_train <- data.frame(dataset='train',RMSE = RMSE(prediction1,train_df$price_usd),Rsquared = R2(prediction1,train_df$price_usd))
mod_train
```
```{r}
prediction1 <- predict(model_lasso,newdata = test_df)
mod_test <- data.frame(dataset='test',RMSE = RMSE(prediction1,test_df$price_usd),Rsquared = R2(prediction1,test_df$price_usd))
new_df_l<-rbind(mod_train,mod_test)
print(new_df_l)

```


```{r}
####################### Ridge regression ##################################
ctrls <- trainControl(method = 'cv',number = 10,savePredictions = 'all')
```

```{r}
lambda_vect <- 10^seq(5,-5,length = 500)
model_ridge  <- train(price_usd~transmission + color + odometer_value + engine_type +
    engine_capacity + body_type + drivetrain + state + number_of_photos +
    feature_7 + feature_3 + feature_6 + feature_5 +
    year_produced + manufacturer_name,data = updated_df,
    method = "glmnet",tuneGrid = expand.grid(alpha = 0,lambda = lambda_vect),trControl = ctrls)
```

```{r}
model_ridge$bestTune
```

```{r}
round(coef(model_ridge$finalModel,model_lasso$bestTune$lambda),3)
```

```{r}
prediction1 <- predict(model_ridge,newdata = train_df)
mod_trainr <- data.frame(dataset='train',RMSE = RMSE(prediction1,train_df$price_usd),Rsquared = R2(prediction1,train_df$price_usd))
mod_trainr
```

```{r}
prediction1 <- predict(model_ridge,newdata = test_df)
mod_testr <- data.frame(dataset='test',RMSE = RMSE(prediction1,test_df$price_usd),Rsquared =R2(prediction1,test_df$price_usd))
new_df1<-rbind(mod_trainr,mod_testr)
print(new_df1)
```